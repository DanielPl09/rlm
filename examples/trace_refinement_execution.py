"""
Traced execution showing actual sub_RLM calls and hypothesis refinement.
This demonstrates the real workflow with instrumented logging.
"""

import os
import sys

# Add parent directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rlm.repl import REPLEnv, Sub_RLM
from rlm.utils.context_slicer import ContextSlicer
import json


class TracedSubRLM(Sub_RLM):
    """
    Instrumented Sub_RLM that logs all calls and responses.
    """

    def __init__(self, *args, **kwargs):
        # Don't call super().__init__ to avoid API key requirement
        # This is a mock for demonstration purposes
        self.model = kwargs.get('model', 'mock-model')
        self.context_slices = kwargs.get('context_slices', {})
        self.call_count = 0

    def completion(self, prompt, ctx_slice_id=None):
        """Mock completion with instrumented logging."""
        self.call_count += 1

        print(f"\n{'='*80}")
        print(f"üîµ sub_RLM Call #{self.call_count}")
        print(f"{'='*80}")

        # Show which slice is being queried
        if ctx_slice_id:
            print(f"üìé Context Slice: {ctx_slice_id}")
            if ctx_slice_id in self.context_slices:
                slice_obj = self.context_slices[ctx_slice_id]
                content_preview = str(slice_obj.content)[:200]
                print(f"üìÑ Slice Content Preview: {content_preview}...")
                print(f"üìä Slice Metadata: {slice_obj.metadata}")
        else:
            print(f"üìé Context Slice: None (full context query)")

        # Show the prompt
        if isinstance(prompt, str):
            print(f"\nüí¨ Prompt:")
            print(f"   {prompt[:300]}{'...' if len(prompt) > 300 else ''}")
        else:
            print(f"\nüí¨ Prompt (messages):")
            for msg in prompt[:2]:  # Show first 2 messages
                role = msg.get('role', 'unknown')
                content = msg.get('content', '')[:150]
                print(f"   [{role}]: {content}{'...' if len(msg.get('content', '')) > 150 else ''}")

        # Simulate response based on slice content
        if ctx_slice_id and ctx_slice_id in self.context_slices:
            slice_obj = self.context_slices[ctx_slice_id]
            response = f"[MOCK RESPONSE] Analysis of {ctx_slice_id}: {str(slice_obj.content)[:100]}..."
        else:
            # This is a refinement query
            if isinstance(prompt, str) and "hypothesis" in prompt.lower():
                response = f"[MOCK REFINEMENT] Refined hypothesis based on new information"
            else:
                response = "[MOCK RESPONSE] General query response"

        print(f"\n‚úÖ Response:")
        print(f"   {response}")
        print(f"{'='*80}\n")

        return response


def trace_hypothesis_evolution():
    """
    Demonstrate hypothesis tracking with traced execution.
    """
    print("\n" + "="*100)
    print("üî¨ TRACED EXECUTION: Hypothesis Evolution")
    print("="*100)

    # Create context
    context = {
        "document_a": "This document discusses feature X which has great performance.",
        "document_b": "This document mentions feature Y which needs improvement.",
        "document_c": "This document shows feature Z which is highly rated by users."
    }

    print(f"\nüìö Context: {len(context)} documents")
    for key in context.keys():
        print(f"   - {key}")

    # Auto-slice the context
    print(f"\nüî™ Auto-slicing context...")
    slices = ContextSlicer.auto_slice_context(context)
    print(f"   Created {len(slices)} slices: {list(slices.keys())}")

    # Create REPLEnv without requiring API key (mock version)
    print(f"\nüèóÔ∏è  Creating REPL environment with slices...")

    # We'll manually trace instead of using actual REPLEnv to avoid API key requirement
    print(f"\n{'='*100}")
    print("üìù REPL Code Execution Trace")
    print(f"{'='*100}")

    # Simulate REPL code execution
    code_step_1 = """
# Discover available slices
slices = list_slices()
print(f"Available slices: {slices}")
"""
    print(f"\n[Code Block 1]")
    print(code_step_1)
    print(f"[Output]")
    print(f"   Available slices: {list(slices.keys())}")

    # Initialize hypothesis
    code_step_2 = """
# Initialize hypothesis
update_hypothesis("Starting analysis: need to examine all documents")
print(f"Initial hypothesis: {get_hypothesis()}")
"""
    print(f"\n[Code Block 2]")
    print(code_step_2)
    print(f"[Output]")
    hypothesis = "Starting analysis: need to examine all documents"
    hypothesis_history = []
    print(f"   Initial hypothesis: {hypothesis}")

    # Create traced sub_rlm
    traced_rlm = TracedSubRLM(model='mock', context_slices=slices)

    # Query first slice
    code_step_3 = """
# Query first slice
result_1 = llm_query(
    "What information is in this document?",
    slice_id="dict_document_a"
)
print(f"Result from document_a: {result_1}")
"""
    print(f"\n[Code Block 3]")
    print(code_step_3)
    print(f"[Output]")
    result_1 = traced_rlm.completion(
        "What information is in this document?",
        ctx_slice_id="dict_document_a"
    )
    print(f"   Result from document_a: {result_1}")

    # Refine hypothesis after first slice
    code_step_4 = """
# Refine hypothesis with first finding
current = get_hypothesis()
refined = llm_query(f'''
    Current hypothesis: {current}
    New finding: {result_1}
    Provide refined hypothesis.
''')
update_hypothesis(refined)
print(f"Hypothesis v1: {get_hypothesis()}")
"""
    print(f"\n[Code Block 4]")
    print(code_step_4)
    print(f"[Output]")
    refinement_prompt = f"Current hypothesis: {hypothesis}\nNew finding: {result_1}\nProvide refined hypothesis."
    refined_1 = traced_rlm.completion(refinement_prompt)
    hypothesis_history.append(hypothesis)
    hypothesis = "After document_a: Found information about feature X with great performance"
    print(f"   Hypothesis v1: {hypothesis}")

    # Query second slice
    code_step_5 = """
# Query second slice
result_2 = llm_query(
    "What information is in this document?",
    slice_id="dict_document_b"
)
print(f"Result from document_b: {result_2}")
"""
    print(f"\n[Code Block 5]")
    print(code_step_5)
    print(f"[Output]")
    result_2 = traced_rlm.completion(
        "What information is in this document?",
        ctx_slice_id="dict_document_b"
    )
    print(f"   Result from document_b: {result_2}")

    # Refine hypothesis after second slice
    code_step_6 = """
# Refine hypothesis with second finding
current = get_hypothesis()
refined = llm_query(f'''
    Current hypothesis: {current}
    New finding: {result_2}
    Provide refined hypothesis.
''')
update_hypothesis(refined)
print(f"Hypothesis v2: {get_hypothesis()}")
"""
    print(f"\n[Code Block 6]")
    print(code_step_6)
    print(f"[Output]")
    refinement_prompt_2 = f"Current hypothesis: {hypothesis}\nNew finding: {result_2}\nProvide refined hypothesis."
    refined_2 = traced_rlm.completion(refinement_prompt_2)
    hypothesis_history.append(hypothesis)
    hypothesis = "After documents A & B: Feature X performs well, Feature Y needs improvement"
    print(f"   Hypothesis v2: {hypothesis}")

    # Query third slice
    code_step_7 = """
# Query third slice
result_3 = llm_query(
    "What information is in this document?",
    slice_id="dict_document_c"
)
print(f"Result from document_c: {result_3}")
"""
    print(f"\n[Code Block 7]")
    print(code_step_7)
    print(f"[Output]")
    result_3 = traced_rlm.completion(
        "What information is in this document?",
        ctx_slice_id="dict_document_c"
    )
    print(f"   Result from document_c: {result_3}")

    # Final refinement
    code_step_8 = """
# Final refinement
current = get_hypothesis()
final = llm_query(f'''
    Current hypothesis: {current}
    New finding: {result_3}
    Provide final comprehensive answer.
''')
update_hypothesis(final)
print(f"Final hypothesis: {get_hypothesis()}")
"""
    print(f"\n[Code Block 8]")
    print(code_step_8)
    print(f"[Output]")
    final_prompt = f"Current hypothesis: {hypothesis}\nNew finding: {result_3}\nProvide final answer."
    final_refined = traced_rlm.completion(final_prompt)
    hypothesis_history.append(hypothesis)
    hypothesis = "Final: Feature X has great performance, Feature Y needs work, Feature Z is highly rated"
    print(f"   Final hypothesis: {hypothesis}")

    # Show hypothesis history
    code_step_9 = """
# View hypothesis evolution
history = get_hypothesis_history()
print(f"Hypothesis history ({len(history)} versions):")
for i, h in enumerate(history):
    print(f"  v{i}: {h}")
"""
    print(f"\n[Code Block 9]")
    print(code_step_9)
    print(f"[Output]")
    print(f"   Hypothesis history ({len(hypothesis_history)} versions):")
    for i, h in enumerate(hypothesis_history):
        print(f"     v{i}: {h}")

    # Summary
    print(f"\n{'='*100}")
    print("üìä EXECUTION SUMMARY")
    print(f"{'='*100}")
    print(f"\n‚úÖ Total sub_RLM calls: {traced_rlm.call_count}")
    print(f"   - Slice queries: 3 (one per document)")
    print(f"   - Refinement queries: 3 (one after each slice)")
    print(f"\nüìà Hypothesis Evolution:")
    print(f"   v0 (initial): Starting analysis...")
    print(f"   v1 (after doc A): Found feature X performance")
    print(f"   v2 (after doc B): Added feature Y needs improvement")
    print(f"   v3 (after doc C): Added feature Z highly rated")
    print(f"\nüéØ Final Answer: Comprehensive analysis of all three features")
    print(f"\n{'='*100}\n")


if __name__ == "__main__":
    trace_hypothesis_evolution()
